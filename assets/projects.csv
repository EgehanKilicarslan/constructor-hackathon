Title,Github URL,Paper URL,Specific URL,Documentation,Dependencies,Special features,Entry Points (Scripts),Execution Command,External Credentials,Dataset Dependency,Terminate
generals-bots,https://github.com/strakam/generals-bots,https://arxiv.org/abs/2507.06825,https://arxiv.org/pdf/2507.06825,README + Wiki. Install/use shown; examples linked.,7.0,"Python-based reinforcement learning environment for the game Generals.io.
Includes multiple agent baselines, replay GUI, and both Gymnasium and PettingZoo wrappers.
Designed for research and experimentation with multi-agent game strategies.
Easily extendable with custom agents and compatible with standard RL toolkits.",examples/complete_example.py (Primary),"PYTHONPATH=. python examples/complete_example.py (source: README “complete example”)
pytest -q (source: tests/ folder)",None (online autopilot requires generals.io account/lobby if used),None (built-in map generator),Yes
gigaam,https://github.com/salute-developers/gigaam,https://arxiv.org/abs/2506.01192,https://arxiv.org/pdf/2506.01192,README in repo,6.0,"Transformer-based Acoustic Model for Speech and Emotion Recognition. Uses Hydra configs, VAD, emotion conditioning. HuggingFace model only.",gigaam (library import) (Primary); inference_example.ipynb,"PYTHONPATH=. python -c ""import gigaam; print(hasattr(gigaam, 'load_model'))"" (source: README usage)
PYTHONPATH=. python -c ""import gigaam; m=gigaam.load_model('rnnt'); print(str(m.transcribe('path/to/audio.wav'))[:80])"" (source: README usage)
PYTHONPATH=. python -c ""import gigaam; m=gigaam.load_model('emo'); print(m.get_probs('path/to/audio.wav'))"" (source: README usage)
export HF_TOKEN=""your_hf_token"" (source: README long-form)
PYTHONPATH=. python -c ""import os,gigaam; os.environ['HF_TOKEN']=os.environ.get('HF_TOKEN',''); m=gigaam.load_model('ctc'); print(list(m.transcribe_longform('long_example.wav'))[:1])"" (source: README long-form)",None; HF token required only for long-form VAD (pyannote models),User-provided audio files (no fixed dataset),Yes
formatron,https://github.com/Dan-wanna-M/formatron/tree/master,https://arxiv.org/abs/2506.01151,https://arxiv.org/pdf/2506.01151,README only; no docs,11.0,"Supports multiple LLM backends (ExLlamaV2, vLLM, Transformers) with structured output control via JSON schema.",formatron (library import) (Primary); examples/,"PYTHONPATH=src python -c ""import formatron; print('formatron imported')"" (source: src/formatron/)
PYTHONPATH=src pytest -q (source: tests/)
PYTHONPATH=src python - <<'PY'\nfrom formatron.formatter import FormatterBuilder\nf=FormatterBuilder(); f.append_line('Hello, World!')\nprint('demo ok')\nPY (source: README → “Examples” pattern)",None (some HF models referenced in README may require access on HuggingFace),,No
cmbagent,https://github.com/CMBAgents/cmbagent,https://arxiv.org/abs/2507.07257,https://arxiv.org/pdf/2507.07257,README + usage instructions,19.0,"LLM-based agent framework for simulating cosmological model analysis. Uses crewAI and ConstructorAdapter to run agents solving tasks like computing H0 parameter. Includes LangChain, LangGraph, and other modern libraries.",cmbagent (console entry) (Primary); backend/run.py; cmbagent.one_shot (library),"PYTHONPATH=. python -c ""import cmbagent; print('cmbagent import ok')"" (source: README → “Run” import)
PYTHONPATH=. python -c ""import cmbagent; task='Draw two random numbers and give me their sum'; print(cmbagent.one_shot(task, agent='engineer', engineer_model='gpt-4o-mini'))"" (source: README “Run”; requires OPENAI_API_KEY)
PYTHONPATH=. python backend/run.py (source: README Manual Setup)
PYTHONPATH=. pytest -q (source: tests/ present)",OPENAI_API_KEY required; optional ANTHROPIC_API_KEY; optional GOOGLE_APPLICATION_CREDENTIALS (Vertex AI),None (agents operate on prompts/RAG content; optional external repo cmbagent_data for RAG),Yes
nanoGPT,https://github.com/karpathy/nanoGPT,https://arxiv.org/pdf/2507.07101,https://arxiv.org/pdf/2507.07101,README,17.0,Lightweight educational GPT training from scratch,train.py (Primary); sample.py; data/shakespeare/prepare.py,"PYTHONPATH=. python data/shakespeare/prepare.py (source: README → Shakespeare data prep)
PYTHONPATH=. python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=10 --block_size=64 --batch_size=8 --n_layer=2 --n_head=2 --n_embd=128 --max_iters=100 (source: README + config/train_shakespeare_char.py; tiny CPU smoke test)
PYTHONPATH=. python train.py config/train_shakespeare_char.py --device=cuda (source: README + config/train_shakespeare_char.py; practical GPU training)
PYTHONPATH=. python sample.py --out_dir=out-shakespeare-char --device=cpu (source: README + sample.py; requires trained checkpoint)",,Downloads Tiny Shakespeare via prepare.py; user-provided text also supported,No
OpenDPD,https://github.com/lab-emi/OpenDPD,https://arxiv.org/abs/2507.06849,https://arxiv.org/pdf/2507.06849,README,8.0,Deep Parametric Decoder for reconstructing speech/audio from features.,main.py (Primary); train_all_pa.sh; train_all_dpd.sh; OpenDPDv2.sh; quant_mp_dpd.sh ,"python main.py --dataset_name DPA_200MHz --step train_pa --accelerator cpu (source: README)
python main.py --dataset_name DPA_200MHz --step train_dpd --accelerator cpu (source: README)
python main.py --dataset_name DPA_200MHz --step run_dpd --accelerator cpu (source: README)
bash train_all_pa.sh (source: README)
bash train_all_dpd.sh (source: README)
bash OpenDPDv2.sh (source: README)
bash quant_mp_dpd.sh (source: README)",,"Bundled PA datasets (e.g., DPA_200MHz, APA_200MHz)",Yes
agibotworld,https://github.com/OpenDriveLab/AgiBot-World,https://arxiv.org/abs/2507.06219,https://arxiv.org/pdf/2507.06219,README in repo,11.0,Multi-agent embodied learning in simulated 3D environments using HuggingFace LeRobot and OpenDriveLab tools; integrates vision-language models with action planning for robotic tasks.,scripts/visualize_dataset.py (Primary); go1/shell/train.sh; evaluate/libero/convert_libero_data_to_lerobot.py; evaluate/deploy.py; evaluate/openloop_eval.py,"python scripts/visualize_dataset.py --task-id 390 --dataset-path /path/to/lerobot/format/dataset (source: README “Visualize Datasets”)
python evaluate/libero/convert_libero_data_to_lerobot.py --data_dir /path/to/your/libero/data (source: README “Prepare Data”)
RUNNAME=<YOUR_RUNNAME> bash go1/shell/train.sh go1/configs/go1_sft_libero.py (source: README “Start Fine-tuning”)
python evaluate/deploy.py --model_path /path/to/checkpoint --data_stats_path /path/to/dataset_stats.json --port 8080 (source: README “Remote Inference”)",,Requires AgiBot-World datasets (Alpha/Beta) and/or LIBERO converted to LeRobot format; optional downloads via HuggingFace/OpenDataLab,Yes
graphrag-toolkit,https://github.com/awslabs/graphrag-toolkit,https://arxiv.org/abs/2507.04127,https://arxiv.org/pdf/2507.04127,README in repo,12.0,"Supports multi-modal RAG pipelines with graph-based chunking and retrieval. Includes tools for evaluation and benchmark integration (e.g., RAGAS), OpenAI/LLM support, and compatibility with LangChain/Streamlit.",graphrag/run_graph_rag_pipeline.py (Primary),"OPENAI_API_KEY=your_key PYTHONPATH=. python graphrag/run_graph_rag_pipeline.py --help (source: README → “Run pipeline”)
OPENAI_API_KEY=your_key PYTHONPATH=. python graphrag/run_graph_rag_pipeline.py (source: README → “Run pipeline”)",OPENAI_API_KEY (and other provider keys if used),User-provided documents/datasets (optional benchmarks via integrations),Yes
pdfmathtranslate,https://github.com/byaidu/pdfmathtranslate,https://arxiv.org/abs/2507.03009,https://arxiv.org/pdf/2507.03009,README in repo,22.0,Provides an end-to-end pipeline for extracting mathematical expressions from PDFs and converting them into LaTeX using GPT-based OCR and formula segmentation. Supports visual parsing and batch processing.,main.py (Primary),"OPENAI_API_KEY=sk-... PYTHONPATH=. python main.py --help (source: README → usage)
OPENAI_API_KEY=sk-... PYTHONPATH=. python main.py --input path/to/file.pdf --output out_dir (source: README → usage)",OPENAI_API_KEY (for GPT-based OCR),User-provided PDFs,Yes
Flash-VStream,https://github.com/IVGSZ/Flash-VStream,https://arxiv.org/abs/2506.23825,https://arxiv.org/pdf/2506.23825,README present,13.0,"Streamlined framework for evaluating multimodal LLMs on video-based tasks. Supports frame extraction, table parsing, and pipeline-based model interaction.",Flash-VStream-LLaVA/realtime_cli.sh (Primary); Flash-VStream-Qwen/realtime_cli.sh,"bash -n Flash-VStream-LLaVA/realtime_cli.sh (source: Flash-VStream-LLaVA/realtime_cli.sh)
bash Flash-VStream-LLaVA/realtime_cli.sh (source: subproject README)
bash -n Flash-VStream-Qwen/realtime_cli.sh (source: Flash-VStream-Qwen/realtime_cli.sh)
bash Flash-VStream-Qwen/realtime_cli.sh (source: subproject README)",,User-provided videos (optional benchmark datasets),Yes
mteb,https://github.com/embeddings-benchmark/mteb,https://arxiv.org/abs/2506.21182,https://arxiv.org/pdf/2506.21182,README in repo,14.0,Benchmarking suite for multilingual text embedding evaluation across 100+ tasks and 112 languages. Includes standardized interface and CLI tools.,mteb (Primary); mteb/cli.py,"PYTHONPATH=. python -c ""import mteb; print(hasattr(mteb, 'MTEB'))"" (source: README → Python API)
PYTHONPATH=. python -m mteb.cli available_tasks (source: README → Using the CLI)
PYTHONPATH=. python -m mteb.cli run -m sentence-transformers/all-MiniLM-L6-v2 -t Banking77Classification --verbosity 3 (source: README → Using the CLI)",None (HF token only if using gated models),Yes — tasks auto-download datasets (can be multiple GB),No
ClearerVoice-Studio,https://github.com/modelscope/ClearerVoice-Studio,https://arxiv.org/abs/2506.19398,https://arxiv.org/pdf/2506.19398,README in repo,17.0,Studio for speech enhancement and denoising using neural vocoders and diffusion. Includes Gradio demo and multiple enhancement pipelines.,clearvoice/demo_Numpy2Numpy.py (Primary); clearvoice (library import),"PYTHONPATH=. python -c ""from clearvoice import ClearVoice; m=ClearVoice(task='speech_enhancement', model_names=['FRCRN_SE_16K']); x=m(input_path='samples/input.wav', online_write=False); m.write(x, output_path='samples/output_FRCRN_SE_16K.wav')"" (source: README → ClearVoice usage)
PYTHONPATH=. python clearvoice/demo_Numpy2Numpy.py (source: example demo script)",,User-provided audio/video files (optional sample assets),Yes
reasoning360,https://github.com/LLM360/Reasoning360,https://arxiv.org/abs/2506.14965,https://arxiv.org/pdf/2506.14965,README,22.0,"Benchmark suite evaluating LLM reasoning across multiple categories and datasets. Includes OpenAI, Mistral, and HuggingFace configs.",examples/eval_reasoning360.py (Primary); scripts/train/example_multinode_rl_qwen2.5_32b_base_fsdp.sh,"PYTHONPATH=. python examples/eval_reasoning360.py --help (source: README → “Evaluation”)
sbatch scripts/train/example_multinode_rl_qwen2.5_32b_base_fsdp.sh (source: README → “RL Training”)
PYTHONPATH=. pytest -q (source: tests/ present)","Optional provider keys (e.g., OPENAI_API_KEY, MISTRAL_API_KEY); HuggingFace token if using gated models",Evaluation datasets auto-download; RL training recommends LLM360/guru-RL-92k from Hugging Face,Yes
FlexRAG,https://github.com/ictnlp/FlexRAG,https://arxiv.org/abs/2506.12494,https://arxiv.org/pdf/2506.12494,README + API,18.0,"Modular Retrieval-Augmented Generation (RAG) framework. Supports flexible retriever/generator combinations, lightweight inference, configurable benchmarks, and document corpora.",python -m flexrag.entrypoints.run_interactive (Primary); python -m flexrag.entrypoints.eval_assistant; python -m flexrag.entrypoints.prepare_retriever; python -m flexrag.entrypoints.add_index; python -m flexrag.entrypoints.cache; python -m flexrag.entrypoints.serve_retriever,"PYTHONPATH=src OPENAI_API_KEY=sk-... python -m flexrag.entrypoints.run_interactive assistant_type=modular modular_config.retriever_type='FlexRAG/enwiki_2021_atlas' modular_config.response_type=original modular_config.generator_type=openai modular_config.openai_config.model_name='gpt-4o-mini' modular_config.openai_config.api_key=$OPENAI_API_KEY modular_config.do_sample=False (source: Docs “Quickstart / Entrypoints”)
PYTHONPATH=src OPENAI_API_KEY=sk-... python -m flexrag.entrypoints.eval_assistant name=nq split=test assistant_type=modular modular_config.retriever_type='FlexRAG/enwiki_2021_atlas' modular_config.response_type=short modular_config.generator_type=openai modular_config.openai_config.model_name='gpt-4o-mini' modular_config.openai_config.api_key=$OPENAI_API_KEY modular_config.do_sample=False eval_config.metrics_type=[retrieval_success_rate,generation_f1,generation_em] log_interval=100 (source: Docs “Evaluate Assistant”)
PYTHONPATH=src python -m flexrag.entrypoints.prepare_retriever input_dir=/path/to/docs output_dir=/path/to/index kind=dense (source: Docs “Prepare Retriever”)
PYTHONPATH=src python -m flexrag.entrypoints.add_index index_dir=/path/to/index (source: Docs “Add Index”)",OPENAI_API_KEY (typical); others optional depending on chosen components,"Uses prebuilt HF retrievers (e.g., FlexRAG/enwiki_2021_atlas) or user-prepared corpora via prepare_retriever/add_index",Yes
FlagEvalMM,https://github.com/flageval-baai/FlagEvalMM,https://arxiv.org/abs/2506.09081,https://arxiv.org/pdf/2506.09081,README + examples,34.0,"Framework for evaluating multi-modal LLMs on vision-language tasks. Supports CLIP, MME, CMMU, SEED-Bench, etc.","Framework for evaluating multi-modal LLMs on vision-language tasks; supports CLIP, MME, CMMU, SEED-Bench and others; runnable demo via main retriever script","PYTHONPATH=. python run_retriever.py --help (source: README/examples)
PYTHONPATH=. python run_retriever.py --model <MODEL_NAME> --dataset <DATASET_NAME> (source: README/examples)","Optional provider keys if using cloud models (e.g., OpenAI/Mistral); Hugging Face token if using gated checkpoints","Requires download of benchmark datasets (e.g., MME, CMMU, SEED-Bench)",Yes
SeerAttention,https://github.com/microsoft/SeerAttention,https://arxiv.org/abs/2506.08889,https://arxiv.org/pdf/2506.08889,README,11.0,"Visual attention analysis toolkit for interpreting transformer-based vision models. Built by Microsoft. Includes demos, evaluation, and visualization.",distillation_prefill.py (Primary); distillation_decode.py ,"PYTHONPATH=. python -c ""from seer_attn import SeerAttnLlamaForCausalLM; import transformers, torch; print('seer_attn import OK')"" (source: README Quick Start)
PYTHONPATH=. python distillation_prefill.py --help (source: distillation_prefill.py)
PYTHONPATH=. python distillation_decode.py --help (source: distillation_decode.py)","Hugging Face token may be required for gated base models (e.g., Meta Llama, DeepSeek)",None fixed; evaluation scripts may download benchmark data as configured,Yes
lightllm,https://github.com/ModelTC/lightllm,https://arxiv.org/abs/2506.03887,https://arxiv.org/pdf/2506.03887,README + scripts,11.0,Lightweight inference library for large language models using FlashAttention2 and Triton.,python -m lightllm.server.api_server (Primary),"PYTHONPATH=. python -m lightllm.server.api_server --model_dir /path/to/Qwen3-8B (source: README “Quick Start”)
curl http://127.0.0.1:8000/generate
 -H ""Content-Type: application/json"" -d '{""inputs"":""What is AI?"",""parameters"":{""max_new_tokens"":17}}' (source: README “Quick Start” endpoint)
PYTHONPATH=. pytest -q test (source: tests/ present)",,None (requires local HF weights in --model_dir),Yes
ShapeLLM-Omni,https://github.com/JAMESYJL/ShapeLLM-Omni/,https://arxiv.org/abs/2506.01853,https://arxiv.org/pdf/2506.01853,README + examples,19.0,"Framework for 3D reconstruction, generation and shape understanding using LLM. Includes ShapeEval, OpenScene, point and mesh format support.",app.py (Primary); examples/ (demo scripts),"PYTHONPATH=. python -c ""import importlib.util; print('ok' if importlib.util.spec_from_file_location('app','app.py') else 'missing')"" (source: README → app.py present)
PYTHONPATH=. python app.py (source: README → Inference)
PYTHONPATH=. pytest -q test_3dvqvae.py (source: tests file present)",None (HF token only if using gated models),None (operates on prompts/images; downloads weights on first run),Yes
GSCodec_Studio,https://github.com/JasonLSC/GSCodec_Studio,https://arxiv.org/abs/2506.01822,https://arxiv.org/pdf/2506.01822,README present,16.0,Neural audio codec framework for generative speech compression with transformer models.,main.py (Primary),"PYTHONPATH=. python main.py --help (source: main.py)
PYTHONPATH=. python -c ""import importlib.util; print('ok' if importlib.util.spec_from_file_location('main','main.py') else 'missing')"" (source: repo layout)",,User-provided audio inputs (no fixed dataset bundled),Yes
CleanS2S,https://github.com/opendilab/CleanS2S,https://arxiv.org/abs/2506.01268,https://arxiv.org/pdf/2506.01268,README + Training Guide,18.0,Framework for clean and efficient SFT for seq2seq models. Introduces high-quality data filtering and LLM-guided revision.,cleans2s_eval.py (Primary),"PYTHONPATH=. python cleans2s_eval.py --help (source: README → Evaluation)
PYTHONPATH=. python -c ""import importlib.util; print('ok' if importlib.util.spec_from_file_location('eval','cleans2s_eval.py') else 'missing')"" (source: repo layout)",None (HF token only if using gated models),User-provided or HF datasets (may auto-download),Yes
OpenPAR,https://github.com/Event-AHU/OpenPAR,https://arxiv.org/abs/2505.23313,https://arxiv.org/pdf/2505.23313,Basic README (no API docs),20.0,"Open-source implementation of 3D pose alignment with detailed modular components for training, evaluation, and visualization; integrates synthetic datasets; supports both transformer and CNN backbones.",main.py (Primary),"PYTHONPATH=. python main.py --help (source: repo layout)
PYTHONPATH=. python -c ""import importlib.util; print('ok' if importlib.util.spec_from_file_location('main','main.py') else 'missing')"" (source: repo layout)",,Synthetic datasets integrated; user datasets may be required for training/eval,Yes
360-LLaMA-Factory,https://github.com/Qihoo360/360-LLaMA-Factory,https://arxiv.org/abs/2505.22296,https://arxiv.org/pdf/2505.22296,README in repo,20.0,"Full-stack LLM training/inference suite; integrates LoRA, QLoRA, DPO, PPO, RAG, chat API & WebUI",train_mem.py (Primary); web UI/app scripts (if present),"PYTHONPATH=. python train_mem.py --help (source: README → training usage)
PYTHONPATH=. python -c ""import importlib.util; print('ok' if importlib.util.spec_from_file_location('train','train_mem.py') else 'missing')"" (source: repo layout)",,User-provided datasets or Hugging Face datasets (as configured),Yes
rStar,https://github.com/microsoft/rStar,https://arxiv.org/abs/2505.21297,https://arxiv.org/pdf/2505.21297,README present,2.0,Lightweight tool for generating synthetic data using regex-based patterns; supports colorful terminal output for better visualization.,examples/chat_with_tool_call.py (Primary); examples/aime_eval.sh; examples/math500_eval.sh; data_preprocess/aime2024_rstar2_agent_loop.py; data_preprocess/dapo_rstar2_agent_loop.py,"PYTHONPATH=. python examples/chat_with_tool_call.py --help (source: README → “Try rStar2-Agent with Tool Calling / Script Options”)
MODEL_PATH=/path/to/model bash examples/aime_eval.sh (source: README → “Evaluation”)
PYTHONPATH=. python data_preprocess/aime2024_rstar2_agent_loop.py (source: README → “Data Preparation”)",None (may require Hugging Face token for gated models),AIME24/25 and MATH500 auto-download or preprocessed via data_preprocess scripts; training example uses DAPO-17k,Yes
FutureMotion,https://github.com/kit-mrt/future-motion,https://arxiv.org/abs/2505.20414,https://arxiv.org/pdf/2505.20414,README + pyproject.toml + examples,6.0,Real-time trajectory prediction for autonomous driving; multiple prediction horizons; uses lightweight model and OpenDRIVE-based scenarios.,predictor.py (Primary); examples/,"PYTHONPATH=. python -c ""import importlib.util; print('ok' if importlib.util.spec_from_file_location('predictor','predictor.py') else 'missing')"" (source: repo layout → predictor.py)
PYTHONPATH=. python predictor.py --help (source: README mentions examples/ and usage around predictor)
PYTHONPATH=. python examples/<example_script>.py (source: examples/ folder)",,Built-in/OpenDRIVE-based scenarios; user-provided scenarios optional,Yes
OpenGait,https://github.com/ShiqiYu/OpenGait,https://arxiv.org/abs/2505.18582,https://arxiv.org/pdf/2505.18582,README in repo,8.0,PyTorch-based gait recognition framework; modular backbone; benchmark-ready,opengait/main.py (Primary); train.sh ,"PYTHONPATH=. python opengait/main.py --cfgs ./configs/gaitbase/gaitbase_da_grew.yaml --phase train (source: opengait/main.py; configs/gaitbase)
PYTHONPATH=. python opengait/main.py --cfgs ./configs/gaitbase/gaitbase_da_grew.yaml --phase test (source: opengait/main.py; configs/gaitbase)
bash train.sh (source: train.sh)",,"Requires prepared gait datasets (CASIA-B, OU-MVLP, GREW, Gait3D); not auto-downloaded",Yes
qiskit-machine-learning,https://github.com/qiskit-community/qiskit-machine-learning,https://arxiv.org/abs/2505.17756,https://arxiv.org/pdf/2505.17756,README + notebooks + examples,35.0,"Quantum-compatible ML toolkit; integrates with Qiskit for quantum kernel methods, variational classifiers, and quantum neural networks.",qiskit_machine_learning (library import) (Primary); docs/tutorials/; examples/,"PYTHONPATH=. python -c ""import qiskit_machine_learning as qml; print(getattr(qml, 'version', 'import_ok'))"" (source: README → library usage)
PYTHONPATH=. pytest -q (source: tests present)",,None (tutorials often use scikit-learn toy datasets),Yes
eval-audio-repr,https://github.com/nttcslab/eval-audio-repr/,https://arxiv.org/abs/2505.15307,https://arxiv.org/pdf/2505.15307,README with basic usage,6.0,Focused benchmark for self-supervised audio representations,inference.py (Primary),"PYTHONPATH=. python inference.py --help (source: README → basic usage)
PYTHONPATH=. python -c ""import importlib.util; print('ok' if importlib.util.spec_from_file_location('inference','inference.py') else 'missing')"" (source: repo layout)",,Requires benchmark datasets prepared in expected folder structure; paths must be set before running,Yes
RD-Agent,https://github.com/microsoft/RD-Agent,https://arxiv.org/abs/2505.15155,https://arxiv.org/pdf/2505.15155,README,14.0,RL framework for dialogue agents with retrieval-augmented memory & dynamic policy updates.,rd_agent_main.py (Primary),"python rd_agent_main.py --help (source: README → “Usage”)
PYTHONPATH=. python -c ""import importlib.util as iu; print('ok' if iu.spec_from_file_location('rd_main','rd_agent_main.py') else 'missing')"" (source: repo root → rd_agent_main.py)",OPENAI_API_KEY and/or other provider keys depending on chosen backend,User-provided or referenced benchmarks as configured; may auto-download depending on scripts,Yes
reasoning-boundary,https://github.com/LightChen233/reasoning-boundary,https://arxiv.org/abs/2505.13307,https://arxiv.org/pdf/2505.13307,README,17.0,"Supports both text-based and multimodal datasets; includes evaluation scripts for various reasoning settings (CoT, Tool-Usage, PoT, Complex-CoT, LtM, MARP); provides reproduction of paper results and custom result evaluation; includes visualization scripts for reasoning boundaries.",evaluate.py (Primary),"PYTHONPATH=. python evaluate.py --help (source: file evaluate.py)
PYTHONPATH=. python evaluate.py --config configs/example.yaml (source: README → “Evaluation”)","Optional provider keys (e.g., OpenAI/Mistral) or Hugging Face token if using gated models",Uses evaluation datasets from configs; may auto-download or require local paths depending on task,Yes
LLM4Decompile,https://github.com/albertan017/LLM4Decompile,https://arxiv.org/abs/2505.12668,https://arxiv.org/pdf/2505.12668,README,21.0,Multi-size LLMs (6.7B–22B) for binary-to-source decompilation; GCC/objdump integration; benchmark datasets; configurable optimization levels; reproducible pipeline.,ghidra/demo.py (Primary); evaluation/ (evaluation scripts); train/ (training scripts),"PYTHONPATH=. python ghidra/demo.py (source: README → “Docker setup” → cd ghidra; python demo.py)
PYTHONPATH=. python - <<'PY'\nimport subprocess, os\nprint('GCC/objdump required; demo uses HF model checkpoints')\nPY (source: README → “Quick Start” notes on GCC/objdump and HF models)",None (Hugging Face token only if using gated checkpoints),Built-in evaluation JSON (decompile-eval); optional large training/eval sets from,Yes
RecAI,https://github.com/microsoft/RecAI,https://arxiv.org/abs/2505.03336,https://arxiv.org/pdf/2505.03336,README,34.0,"Modular recommender system framework by Microsoft; pipelines, datasets, notebooks; integration with HuggingFace PEFT/Transformers; reproducible shell scripts and utilities.",main.py (Primary); scripts/; examples/; notebooks/,"PYTHONPATH=. python -c ""import importlib.util; print('ok' if importlib.util.spec_from_file_location('main','main.py') else 'missing')"" (source: repo root layout)
PYTHONPATH=. python main.py --help (source: README → usage)
PYTHONPATH=. pytest -q (source: tests/ present)",None (some pipelines may require provider API keys depending on configuration),User-provided datasets or auto-downloaded benchmarks as configured in examples/scripts,Yes
SigmaRL,https://github.com/bassamlab/SigmaRL,https://arxiv.org/abs/2505.02395,https://arxiv.org/pdf/2505.02395,README in repo,16.0,"Multi-agent reinforcement learning framework with VMAS integration; customizable road scenarios (intersections, roundabouts, merges); safety-critical RL via control barrier functions; supports parallelized simulation and training.",train.py (Primary),"PYTHONPATH=. python train.py --help (source: README → “Usage”)
PYTHONPATH=. python -c ""import importlib.util as iu; print('ok' if iu.spec_from_file_location('train','train.py') else 'missing')"" (source: file path)",,None (uses simulated scenarios; user-configured maps),Yes
CAMELTrack,https://github.com/TrackingLaboratory/CAMELTrack,https://arxiv.org/abs/2505.01257,https://arxiv.org/pdf/2505.01257,README in repo,14.0,"Transformer-based visual tracking framework with CAMEL-based memory module; supports multi-object tracking benchmarks; modular design for dataset loading, model training, and evaluation.",train.py (Primary),"PYTHONPATH=. python train.py --help (source: README → usage)
PYTHONPATH=. python train.py --config configs/<your_config>.yaml (source: README → training/eval pattern)",,Requires prepared tracking datasets per config (benchmark datasets not auto-downloaded),Yes
RAGEN,https://github.com/RAGEN-AI/RAGEN,https://arxiv.org/abs/2504.20073,https://arxiv.org/pdf/2504.20073,README + YAML configs (Hydra/OMC stack),29.0,"Hydra-based configs; multi-backend LLM stack (Transformers/PEFT, Anthropic, planned vLLM); logging via W&B; optional Ray tooling. Based on sets of packages that were installed/checked at startup.",train.py (Primary); scripts/train_all.sh; scripts/setup_ragen.sh,"python train.py (source: Docs “Quick Start Guide” → Training Script)
python train.py trainer.experiment_name=sokoban-ppo-rolloutfilter0.25 actor_rollout_ref.rollout.rollout_filter_ratio=0.25 (source: Docs “Quick Start Guide” → example override)
bash scripts/train_all.sh (source: Docs “Quick Start Guide” → standard experiments)",W&B optional (wandb login); Hugging Face token only if using gated models,Auto-downloads environment assets via setup script; no large fixed dataset,Yes
vision6D,https://github.com/InteractiveGL/vision6D,https://arxiv.org/abs/2504.15329,https://arxiv.org/pdf/2504.15329,README,11.0,"Supports 6D object pose estimation with deep learning and OpenGL-based visualization; includes dataset loaders, evaluation scripts, and rendering tools.",main.py (Primary),"PYTHONPATH=. python -c ""import importlib.util as iu; print('ok' if iu.spec_from_file_location('main','main.py') else 'missing')"" (source: repo layout)
PYTHONPATH=. python main.py --help (source: README → usage pattern with main script)",,Requires prepared 6D pose datasets per configuration; not auto-downloaded,Yes
UFO,https://github.com/microsoft/UFO/,https://arxiv.org/abs/2504.14603,https://arxiv.org/pdf/2504.14603,README,10.0,Implements a unified object detection framework supporting multiple vision tasks; modular configuration with YAML; pretrained model loading and evaluation pipelines.,main.py (Primary),"PYTHONPATH=. python main.py --help (source: README)
PYTHONPATH=. python main.py --config configs/<your_config>.yaml --mode eval --weights /path/to/checkpoint (source: README)",,"User-provided detection datasets per config (e.g., COCO-style); weights may auto-download if configured",Yes
VoxCity,https://github.com/kunifujiwara/VoxCity,https://arxiv.org/abs/2504.13934,https://arxiv.org/pdf/2504.13934,README + ReadTheDocs,36.0,"Seamless framework for integrating open geospatial data and generating grid/voxel 3D city models; auto-downloads buildings/land cover/canopy/DEM; simple simulations (solar, view indices); exports ENVI-met (INX/EDB), MagicaVoxel (VOX), and OBJ.",quick_test_rome.py (Primary),PYTHONPATH=. python quick_test_rome.py (source: README → quick test),,Auto-downloads open geospatial datasets (buildings/land cover/canopy/DEM); may be multi-GB,Yes
LearningHumanoidWalking,https://github.com/rohanpsingh/LearningHumanoidWalking,https://arxiv.org/abs/2504.13619,https://arxiv.org/pdf/2504.13619,README + CLI usage 14 PPO trainer,14.0,"PPO humanoid locomotion with SymmetricEnv mirroring, Ray-based parallel rollouts, MJCF auto-builder for Unitree H1, YAML configs, optional LSTM policy, built-in eval/video logging, supports JVRC & H1.",run_experiment.py (Primary); scripts/debug_stepper.py,"python run_experiment.py train --logdir exp_runs/demo --num_procs 4 --env jvrc_walk (source: README → “To train”)
python run_experiment.py eval --logdir path/to/actor_checkpoint.pt (source: README → “To play”)
PYTHONPATH=.:$PYTHONPATH python scripts/debug_stepper.py --path exp_runs/demo (source: README → debug_stepper for jvrc_step)",,None (simulated environments; MJCF assets under models/),Yes
RuleKit,https://github.com/adaa-polsl/RuleKit,https://arxiv.org/abs/2504.20650,https://arxiv.org/pdf/2504.20650,README + Wiki 4 Interpretable rule-learning toolkit,8.0,"Interpretable rule-based models (readable IF–THEN rules); scikit-learn-style API (fit/predict); pandas-friendly; multiple rule quality measures (C2, Correlation, etc.) for induction/pruning/voting; lightweight, no GPU required",rulekit_quicktest.py (Primary); rulekit (library import),"PYTHONPATH=. python rulekit_quicktest.py (source: file path)
PYTHONPATH=. python -c ""import rulekit; print('rulekit imported')"" (source: package path)",,None (examples use small in-memory/toy datasets),Yes
MQT QECC,https://github.com/munich-quantum-toolkit/qecc,https://arxiv.org/abs/2504.10591,https://arxiv.org/pdf/2504.10591,README + CLI,14.0,"CLI cc-decoder with two backends (tensor-network tn and MaxSAT), supports hexagon and square-octagon lattices, runs Monte Carlo trials via --nr_sims, writes JSON results to --results_dir, and lets you plug in an external MaxSAT solver with --solver.",python -m mqt.qecc.cc_decoder.cli (Primary); mqt.qecc.cc-decoder,"PYTHONPATH=. python -m mqt.qecc.cc_decoder.cli --help (source: Docs “LightsOut decoder” → CLI help)
PYTHONPATH=. python -m mqt.qecc.cc_decoder.cli 21 0.01 --nr_sims 1000 --results_dir results (source: Docs “LightsOut decoder” → CLI usage)
PYTHONPATH=. python -m mqt.qecc.cc_decoder.cli 11 0.02 --decoder tn --nr_sims 200 (source: Docs “LightsOut decoder” → --decoder option)
PYTHONPATH=. python -m mqt.qecc.cc_decoder.cli 7 0.05 --solver /path/to/rc2 (source: Docs “LightsOut decoder” → --solver option)",,None (simulated experiments; writes results to directory),Yes
octgpt,https://github.com/octree-nn/octgpt,https://arxiv.org/abs/2504.09975,https://arxiv.org/pdf/2504.09975,README,8.0,Octree-based 3D tokenization with PyTorch pipeline; SDF sampling utility (tools/sample_sdf.py) for geometry-aware inputs; OBJ/PLY mesh ingestion via trimesh/plyfile; simple CLI runner (main_octgpt.py); YAML-driven configs; CPU-only path works; lightweight local preprocessing.,main_octgpt.py (Primary); tools/sample_sdf.py,"PYTHONPATH=. python main_octgpt.py --help (source: README)
PYTHONPATH=. python tools/sample_sdf.py --input path/to/mesh.obj --output out_dir (source: file path)
PYTHONPATH=. python main_octgpt.py --config configs/<your_config>.yaml (source: config-driven runner)",,User-provided meshes (OBJ/PLY); optional SDF generation output directory,Yes
TensorNEAT,https://github.com/EMI-Group/tensorneat,https://arxiv.org/abs/2504.08339,https://arxiv.org/pdf/2504.08339,Basic README + examples,15.0,"AX-native NEAT pipeline with vectorized/JIT evolution, deterministic PRNG seeding, modular genome/selection operators, built-in toy benchmarks (e.g., XOR3d), optional Brax/Gymnax env support, CPU-friendly with accelerator support when available.",examples/run_tensorneat_xor.py (Primary); examples/ (other demos),"PYTHONPATH=. JAX_PLATFORM_NAME=cpu python examples/run_tensorneat_xor.py (source: README → examples; file path)
PYTHONPATH=. JAX_PLATFORM_NAME=cpu python examples/run_tensorneat_xor.py --seed 0 --pop_size 64 --generations 50 (source: examples/run_tensorneat_xor.py)
PYTHONPATH=. python -c ""import tensorneat; print('import_ok')"" (source: package import check)",,None (toy tasks bundled; optional Brax/Gymnax environments if used),Yes
llm4ranking,https://github.com/liuqi6777/llm4ranking,https://arxiv.org/abs/2504.07439,https://arxiv.org/pdf/2504.07439,README,13.0,Instructional ranger; easy to replace HF encoders; plug-and-play with BM25/embeddings; CPU inference out of the box; fixed seed,main.py (Primary),"PYTHONPATH=. python main.py --help (source: README → usage)
PYTHONPATH=. python main.py --config configs/example.yaml --retriever bm25 --encoder sentence-transformers/all-MiniLM-L6-v2 (source: README → usage pattern)",None (Hugging Face token only if using gated models),First run may download model weights/datasets from Hugging Face; otherwise user-provided corpora,Yes
"building-assistance-game
",https://github.com/cassidylaidlaw/minecraft-building-assistance-game,https://arxiv.org/abs/2504.07091,https://arxiv.org/pdf/2504.07091,README,12.0,"CraftAssist-based game environment, ""builder assistant"" mode (language prompts → actions), RLlib integration (multi-process experience collection), ready-made scenarios/building schemes and success metrics",mbag/scripts/evaluate.py (Primary); mbag/scripts/train.py; mbag/scripts/evaluate_human_modeling.py,"pytest -m ""not uses_malmo and not slow"" (source: README → “Linting, testing, and type checking”)
PYTHONPATH=. python -m mbag.scripts.train with bc_human data_split=human_alone (source: README → “Running experiments”)
PYTHONPATH=. python -m mbag.scripts.evaluate with human_alone assistant_checkpoint=data/assistancezero_assistant/checkpoint_002000 num_simulations=1 (source: README → “Running assistants in Minecraft”)",,Optional craftassist.zip house dataset (auto-download via README commands) for training/eval; pure tests can run without Malmo/dataset,Yes
Agent-S,https://github.com/simular-ai/Agent-S,https://arxiv.org/abs/2504.00906,https://arxiv.org/pdf/2504.00906,README + examples,19.0,"GUI automation via Agent-Computer Interface (ACI); cross-platform (macOS/Windows/Linux); Python SDK/CLI; plugs into many LLM providers; supports grounding models (e.g., UI-TARS); strong OSWorld-Verified results; easy pip setup and presets.",main.py (Primary); examples/,"PYTHONPATH=. python main.py --help (source: README → CLI usage pattern)
PYTHONPATH=. python -c ""import importlib.util as iu; print('ok' if iu.spec_from_file_location('main','main.py') else 'missing')"" (source: repo layout)","Provider API keys as configured (e.g., OPENAI_API_KEY, ANTHROPIC_API_KEY) depending on chosen preset/model",,Yes
CrackSQL,https://github.com/weAIDB/CrackSQL,https://arxiv.org/abs/2504.00882,https://arxiv.org/pdf/2504.00882,README,13.0,"Hybrid SQL dialect translation with three modes (Rule-only, LLM-direct, Rule+LLM); rules built on SQLGlot/ANTLR with a function-oriented, local→global strategy for cross-dialect syntax; coverage for many dialects plus hybrid support for major RDBMS (PostgreSQL/MySQL/Oracle); multiple interfaces (Python API, CLI, Web UI) and optional GPT/HF backends.",app.py (Primary); cracksql/cli.py; cracksql (library import),"PYTHONPATH=. python app.py --help (source: README → Web UI)
PYTHONPATH=. python -m cracksql.cli --help (source: README → CLI)
PYTHONPATH=. python -c ""import cracksql; print('import_ok')"" (source: README → Python API)",Optional OPENAI_API_KEY / HUGGINGFACE_API_KEY for LLM modes,None (operates on user SQL strings; optional DB connection for Web/backend flow),Yes
xLAM,https://github.com/SalesforceAIResearch/xLAM,https://arxiv.org/abs/2503.22673,https://arxiv.org/pdf/2503.22673,README + ActionStudio_README.md,22.0,"Unified agent-trajectory schema with a generic dataloader; ActionStudio training recipes (full fine-tune, LoRA, distributed with Ray/DeepSpeed); xLAM models compatible with Hugging Face (Transformers/vLLM); tool-call parser plugin for function-calling and multi-turn agent demos; ready-to-use scripts and sample configs.",main.py (Primary); scripts/* (training/launchers),"PYTHONPATH=. python main.py --help (source: README → usage)
bash scripts/train_lora.sh (source: ActionStudio_README.md → training recipes)",Hugging Face token required if using gated model weights,User-provided/curated agent trajectories via unified schema; examples/configs may reference HF-hosted weights/data,Yes
TerraTorch,https://github.com/IBM/terratorch,https://arxiv.org/abs/2503.20563,https://arxiv.org/pdf/2503.20563,README + docs,21.0,"A set for fine-tuning geospatial FM: config/CLI pipelines (Hydra + PyTorch Lightning), integrations with TorchGeo/segmentation-models-pytorch/timm, GeoTIFF support via xarray/rioxarray/GeoPandas, ready-made recipes for classification/segmentation, experiment tracking in MLflow.",main.py (Primary),"PYTHONPATH=. python main.py --help (source: README → CLI patterns)
PYTHONPATH=. python main.py task=classification data.root=/path/to/geotiffs trainer.max_epochs=1 (source: Docs → Quickstart)
PYTHONPATH=. python main.py task=segmentation data.root=/path/to/geotiffs model.backbone=resnet34 trainer.max_epochs=1 (source: Docs → Tasks/Configs)",,User-provided geospatial rasters (GeoTIFF/COGs); some examples may download small open datasets,Yes
AgML,https://github.com/Project-AgML/AgML,https://arxiv.org/abs/2503.20068,https://arxiv.org/pdf/2503.20068,README + docs,12.0,"Unified API for agro-datasets (classification/detection/segmentation); ready-made loaders, markup/metadata and split management; utilities for downloading and normalizing annotations; integration with Albumentations/OpenCV/Scikit-learn; Jupyter widgets (ipywidgets) for fast interactive selection/preview.",agml (Primary); examples/,"PYTHONPATH=. python -c ""import agml; print('agml import OK:', hasattr(agml,'data'))"" (source: README → Quick Start import)
PYTHONPATH=. python -c ""import agml; print('datasets:', len(agml.data.public_data_sources()))"" (source: Docs → Public Dataset Listing)",,Auto-downloads public agricultural datasets on first use; can be multi-GB depending on selection,Yes
Halton-MaskGIT,https://github.com/valeoai/Halton-MaskGIT,https://arxiv.org/abs/2503.17076,https://arxiv.org/pdf/2503.17076,README,28.0,Drop-in Halton scheduler for MaskGIT; uniform token release (quasi-random) for quality/diversity improvement without overtraining.,halton_maskgit_infer.py (Primary); main.py,"PYTHONPATH=. python halton_maskgit_infer.py --help (source: file path)
PYTHONPATH=. python halton_maskgit_infer.py --config configs/example.yaml --ckpt_path /path/to/maskgit_checkpoint.pt --input_dir samples/ --output_dir out/ (source: README pattern + file path)
PYTHONPATH=. python main.py --config configs/train_example.yaml (source: file path)",,User-provided images and pretrained MaskGIT/VQ models; no fixed dataset auto-download,Yes
LLaVA-MORE,https://github.com/aimagelab/LLaVA-MORE,https://arxiv.org/abs/2503.15621,https://arxiv.org/pdf/2503.15621,README,12.0,"ready integration with HF (transformers/accelerate/datasets), PyTorch 2.2.2 + torchvision (CPU wheels), support for timm/einops/sentencepiece, utilities for pre-/post-processing (opencv-headless, pandas, pyarrow), you can import and run light tests on CPU.",src/llava/eval/run_llava.py (Primary); scripts/llava-more/release_1/12_finetuning_llama_31_acc_st_1.sh; scripts/llava-more/release_1/11_pretrain_llama_31_acc_st_1.sh; scripts/llava-more/eval/lmms_eval_single_task.sh,"PYTHONPATH=. python -u src/llava/eval/run_llava.py --model-path aimagelab/LLaVA_MORE-llama_3_1-8B-finetuning --model-architecture llama_3_1 --conv-mode llama_3_1 (source: README → “Inference”)
sbatch scripts/llava-more/release_1/12_finetuning_llama_31_acc_st_1.sh (source: README → “Training”)
sbatch scripts/llava-more/eval/lmms_eval_single_task.sh (source: README → “Evaluation”)",Optional HF token (HF_TOKEN) if using gated models,None fixed; evaluation may auto-download benchmarks via lmms-eval,Yes
VenusFactory,https://github.com/ai4protein/VenusFactory,https://arxiv.org/abs/2503.15438,https://arxiv.org/pdf/2503.15438,README,27.0,"Unified platform for protein engineering: fast dataset collection/curation, reproducible benchmarks and plug-and-play fine-tuning/inference of protein LM; there is a CLI and a simple web interface (Gradio); config-driven pipelines.","src/webui.py (Primary, Gradio UI); script/train/train_plm_vanilla.sh; script/train/train_plm_ses-adapter.sh; script/train/train_plm_adalora.sh; script/train/train_plm_qlora.sh; script/train/train_plm_lora.sh; script/train/train_plm_dora.sh; script/train/train_plm_ia3.sh; script/eval/eval.sh","python ./src/webui.py (source: README → “Quick Start with Venus Web UI”)
bash ./script/train/train_plm_lora.sh (source: README → “Code-line Usage” → LoRA)
bash ./script/eval/eval.sh (source: README → “Code-line Usage” → Basic Evaluation)",None (W&B optional for logging; HF token only for gated models),User-provided or auto-downloads from Hugging Face (multiple GB possible),Yes
SVD-LLM,https://github.com/AIoT-MLSys-Lab/SVD-LLM,https://arxiv.org/abs/2503.12340,https://arxiv.org/pdf/2503.12340,README with Quick Start,9.0,Truncation-aware data whitening + SVD; sequential low-rank updates (LoRA); integration with GPTQ-4bit; scripts for PPL/efficiency.,SVDLLM.py (Primary); compress_llama.sh; svdllm_gptq.sh; quant_llama.py; evaluater.py,"bash compress_llama.sh (source: README → “Quick Example”)
PYTHONPATH=. python SVDLLM.py --step 1 --ratio 0.2 --model HUGGINGFACE_MODEL_REPO --whitening_nsamples 128 --dataset c4 --seed 0 --model_seq_len 2048 --save_path out/whitening (source: README → “Step 1”)
PYTHONPATH=. python SVDLLM.py --step 4 --model_path out/compressed (source: README → “Evaluation → Perplexity”)
PYTHONPATH=. python SVDLLM.py --step 5 --model_path out/compressed (source: README → “Evaluation → Efficiency”)
bash svdllm_gptq.sh (source: README → “SVD-LLM + GPTQ”)",Hugging Face token only for gated model weights (otherwise none),"Whitening/eval use text corpora (e.g., c4 JSONs placed under utils/ per README); LoRA example auto-downloads yahma/alpaca-cleaned; models fetched from Hugging Face",Yes
DeTikZify,https://github.com/potamides/DeTikZify,https://arxiv.org/abs/2503.11509,https://arxiv.org/pdf/2503.11509,README,13.0,"There is a lightweight Gradio-WebUI with d&d/URL and preview, pure TikZ export to fig.tex and PDF/PNG assembly if TeX is available, Python API/CLI (DetikzifyPipeline), CPU/GPU operation with HF_HOME caching, PDF autocropping/preview and local/remote image support.",detikzify/webui.py (Primary; module runner detikzify.webui); examples/*.py,"PYTHONPATH=. python -m detikzify.webui --light (source: README → “web UI”)
PYTHONPATH=. python -c ""from detikzify.model import load; from detikzify.infer import DetikzifyPipeline; pipe=DetikzifyPipeline(*load(model_name_or_path='nllg/detikzify-v2.5-8b', device_map='auto')); fig=pipe.sample(image='https://w.wiki/A7Cc
'); print('tikz_len', len(str(fig.tikz)) if hasattr(fig,'tikz') else 'ok')"" (source: README → “DeTikZify Example”)",,None (operates on user images/URLs; caches models via Hugging Face),Yes
Falcon,https://github.com/TianHuiLab/Falcon,https://arxiv.org/abs/2503.11070,https://arxiv.org/pdf/2503.11070,README,20.0,"single-script inference launch (inference.py, single_gpu_inference_eval.py) for describing satellite images; there is a retrieval module for hints in answers; works with LLM via OpenAI-compatible API (you can substitute OpenAI/Qwen); demo assets; acceleration via flash-attn.",inference.py (Primary); single_gpu_inference_eval.py; multi_node_distributed_train.py,"python inference.py --checkpoint_path <checkpoint_path> --image_path image_samples/IMG_CLS/[IMG_CLS]_003_AID_3525_river_192_ori.png --post_process_type IMG_CLS --prompt ""Classify the image.\nUse one or a few words."" (source: README → “Inference on 14 tasks”)
python inference.py --checkpoint_path <checkpoint_path> --image_path image_samples/REG_DET_OBB/[REG_DET_OBB]_034_DOTA2.0_77716_P0799_ori.png --post_process_type REG_DET_OBB --prompt ""Detect all harbor in the image.\nUse oriented bounding boxes."" (source: README → “Inference on 14 tasks”)",Hugging Face token may be required for model checkpoints,None (sample images included under image_samples/; no fixed dataset),Yes
PyGDA,https://github.com/pygda-team/pygda,https://arxiv.org/abs/2503.10284,https://arxiv.org/pdf/2503.10284,README + docstrings,4.0,Library for Graph Domain Adaptation; includes AirportDataset; models include PairAlign; runs on CPU; uses PyTorch+PyG stack; supports CVXPY optimization via external solvers.,examples/.py (Primary),"PYTHONPATH=. python -c ""from pygda.models import A2GNN; print('A2GNN OK')"" (source: Docs “Quick Start”)
cd benchmark/node && ./run.sh (source: Docs “Benchmarks → Running the Benchmark”)",,User-provided/local datasets; benchmark scripts expect datasets to be present or prepared; no auto-download for many sets,Yes
Open-Sora,https://github.com/hpcaitech/Open-Sora,https://arxiv.org/abs/2503.09642,https://arxiv.org/pdf/2503.09642,README + docs,12.0,End-to-end text-to-video diffusion pipeline (training & inference); Colossal-AI integration for distributed/FP16 training; modular YAML configs with dataset & schedule presets; video I/O helpers (ffmpeg) and sampling scripts.,scripts/diffusion/inference.py (Primary),"torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/t2i2v_256px.py --save-dir samples --prompt ""raining, sea"" (source: README → Quickstart “Text-to-Video Generation”)
torchrun --nproc_per_node 1 --standalone scripts/diffusion/inference.py configs/diffusion/inference/256px.py --cond_type i2v_head --prompt ""A panda in the forest"" --ref assets/texts/i2v.png (source: README → Image-to-Video Generation)",Optional Hugging Face token for model downloads; optional OPENAI_API_KEY for prompt-refine/motion-score evaluator,None for inference (uses prompts and pretrained checkpoints); training requires large user-provided datasets,Yes
RFUAV,https://github.com/kitoweeknd/RFUAV/,https://arxiv.org/abs/2503.09033,https://arxiv.org/pdf/2503.09033,README,21.0,RF-IQ → spectrograms (STFT/Mel) with visualization; scripts for UAV detection/classification and metrics evaluation; modular train/infer configs and notebooks; PyTorch baseline + preprocessing/augmentation utilities.,main.py (Primary); scripts/ (train/infer/eval utilities),"PYTHONPATH=. python main.py --help (source: README)
PYTHONPATH=. python main.py --config configs/example.yaml --mode infer (source: README “inference” pattern)",,Requires prepared datasets with correct paths in configs; no auto-download,Yes
napari-nninteractive,https://github.com/MIC-DKFZ/napari-nninteractive,https://arxiv.org/abs/2503.08373,https://arxiv.org/pdf/2503.08373,README,21.0,"napari plugin for interactive segmentation with scribbles/brush and instant feedback; 2D/3D medical images support; nnUNetv2 and napari-toolkit integration; model loading/cache via HuggingFace Hub; NIfTI (napari-nifti) reading, modular plugin architecture.",napari (Primary; plugin widget “nnInteractive” via Plugins menu); napari -w napari-nninteractive (auto-start widget),"napari -w napari-nninteractive (source: README → “Getting Started”)
napari /path/to/volume.nii.gz -w napari-nninteractive (source: README → “Getting Started”)
napari (then Plugins → nnInteractive) (source: README → “Getting Started”)",None (models auto-download from Hugging Face; token not required unless using gated models),"User-provided medical images (e.g., NIfTI .nii/.nii.gz); no fixed dataset bundled",Yes
ImageFolde,https://github.com/lxa9867/ImageFolder,https://arxiv.org/abs/2503.08354,https://arxiv.org/pdf/2503.08354,README,21.0,utilities for reorganizing datasets to the torchvision.datasets.ImageFolder format; CLI for creating train/val/test splits and generating class_to_idx; support for copying/moving/symlinks and filtering by extensions; compatibility with DataLoader/torchvision pipelines; simple assistants for checking the structure of a dataset.,inference.py (Primary); train.py; trainer.py; sample_imagenet_rar.py,"torchrun --nproc_per_node=2 inference.py --infer_ckpt /path/to/checkpoint --data_path /path/to/ImageNet --depth 17 --encoder_model vit_base_patch14_dinov2.lvd142m --decoder_model vit_base_patch14_dinov2.lvd142m --product_quant 2 --semantic_guide dinov2 --num_latent_tokens 121 --v_patch_nums 1 1 2 3 3 4 5 6 8 11 --pn 1_1_2_3_3_4_5_6_8_11 --patch_size 11 --sem_half True --cfg 3.25 --top_k 750 --top_p 0.95 (source: README → “Inference code for VAR”)
torchrun --nproc_per_node=8 train.py --bs 768 --alng 1e-4 --fp16 1 --wpe 0.01 --tblr 8e-5 --data_path /path/to/ImageNet2012/ --encoder_model vit_base_patch14_dinov2.lvd142m --decoder_model vit_base_patch14_dinov2.lvd142m --product_quant 2 --semantic_guide dinov2 --num_latent_tokens 121 --v_patch_nums 1 1 2 3 3 4 5 6 8 11 --pn 1_1_2_3_3_4_5_6_8_11 --patch_size 11 --vae_ckpt /path/to/ckpt.pt --sem_half True (source: README → “Training code for VAR”)",Optional Hugging Face token for gated checkpoints,"User-provided datasets (e.g., ImageNet); inference evaluation optionally downloads OpenAI VIRTUAL_imagenet npz",Yes
YOLOE,https://github.com/THU-MIG/yoloe,https://arxiv.org/abs/2503.07465,https://arxiv.org/pdf/2503.07465,README,4.0,"YOLO stack based on Ultralytics; integration of CLIP / MobileCLIP / OpenCLIP as backbones; support for COCO/LVIS with ready-made train/eval scripts and config drive; third_party modules (lvis-api, mobileclip, CLIP) are connected from the repository; utilities for inference, visualization and metrics calculation.",main.py (Primary),"PYTHONPATH=. python main.py --help (source: README)
PYTHONPATH=. python main.py --config configs/coco/yoloe_s.yaml --task train (source: README usage pattern)
PYTHONPATH=. python main.py --config configs/lvis/yoloe_lvis.yaml --task val (source: README usage pattern)",,Local COCO/LVIS datasets with correct paths in configs (no auto-download),Yes
InsightFace,https://github.com/deepinsight/insightface,https://arxiv.org/abs/2503.07091,https://arxiv.org/pdf/2503.07091,README,26.0,"a full-featured face analysis stack — detection (RetinaFace), verification/recognition (ArcFace), landmarks/pose/3D and attributes; two backends (PyTorch and ONNXRuntime, CPU-compatible); a large zoo of pre-trained weights and sample scripts/pipelines; convenient Python APIs for inference and export.",insightface; python-package/examples/,"PYTHONPATH=python-package python -c ""import insightface; from insightface.app import FaceAnalysis; app=FaceAnalysis(providers=['CPUExecutionProvider']); app.prepare(ctx_id=0, det_size=(640,640)); print('FaceAnalysis OK:', bool(app.models))"" (source: README Quick Start import and example pattern)",,Downloads pretrained weights on first use; user-provided images/videos required for demos,Yes
K-Radar,https://github.com/kaist-avelab/K-Radar,https://arxiv.org/abs/2503.07029,https://arxiv.org/pdf/2503.07029,README,11.0,"tools for 4D radar dataset (annotation parsing, converters), calibration and mutual projections radar↔camera↔LiDAR, visualization scripts (Open3D/Qt) and evaluations, convenient helpers for synchronizing sensors and preparing samples.",main_train_0.py; main_test_0.py; main_vis.py; main_labeling.py; main_cond_0.py ,"PYTHONPATH=. python main_vis.py --help (source: repo root script)
PYTHONPATH=. python main_train_0.py --config configs/example.yaml --data_root /path/to/K-Radar (source: repo root script + configs/ directory; dataset docs)",,"Requires K-Radar dataset (large, up to ~15TB; subset via Google Drive or shipped HDD per docs); user must set data paths",Yes
AtomThink,https://github.com/Kun-Xiang/AtomThink,https://arxiv.org/abs/2503.06252,https://arxiv.org/pdf/2503.06252,README,19.0,research toolkit for reasoning/LLM pipelines; ready-made train/infer scripts and config-drive (accelerate/DeepSpeed); integration with Hugging Face (hub/weights) and OpenAI API; vision stack via timm/mmcv + preprocessing/estimation utilities; reproducible environments via hard pins.,src/train.py; src/llamafactory/evaluation/run_evaluation_parallel.py,"pip install -r requirements.txt && python -c ""import os; os.environ.setdefault('OPENAI_API_KEY','sk-xxx')"" && torchrun --nproc_per_node 8 --master_addr $master_addr --nnodes $nnode --node_rank $node_rank --master_port $port src/train.py configs/train_full/llama32-11b-vision/llava100k_amath126k_clean_epoch1_2e6.yaml (source: README Quick Start → Start training)
python src/llamafactory/evaluation/run_evaluation_parallel.py --node_rank $node_rank --total_gpus $total_gpus --nproc_per_node 8 --temperature 0.0 --tasks_per_gpu 1 --config ""configs/train_full/llama32-11b-vision/llava100k_amath126k_clean_epoch1_2e6.yaml"" --task MathVision --prompt slow --method slow --atomthink_beam_search_num 2 --candidate_num 3 --max_sampling_count 300 (source: README Quick Start → Start evaluating)",OpenAI API key,Uses AMATH-SFT and other multimodal data via Hugging Face; large model checkpoints required; user must set paths,Yes
Linear-MoE,https://github.com/OpenSparseLLMs/Linear-MoE,https://arxiv.org/abs/2503.05447,https://arxiv.org/pdf/2503.05447,README,23.0,Linearized Mixture-of-Experts for sparse LLMs; PyTorch + Hugging Face (transformers/accelerate); config-drive training/inference scripts; custom CUDA core grouped_gemm for fast routing of experts.,examples/linear_moe_qwen2/run_pretrain_qwen.sh; examples/linear_moe_qwen2/run_finetune_qwen.sh,"bash examples/linear_moe_qwen2/run_pretrain_qwen.sh (source: README Usage → “Start pretraining by sh run_pretrain_qwen.sh”)
bash examples/linear_moe_qwen2/run_finetune_qwen.sh (source: README Usage → “Start finetuning by sh run_finetune_qwen.sh”)",,"Requires large text datasets (e.g., Qwen-formatted) and base checkpoints; user must set DATASET_PATH and PRETRAIN_CHECKPOINT_PATH in scripts",Yes
Slow_Thinking_with_LLMs,https://github.com/RUCAIBox/Slow_Thinking_with_LLMs,https://arxiv.org/abs/2503.04548,https://arxiv.org/pdf/2503.04548,README,22.0,RL slow-thinking (STILL-3); code-integrated/tool-augmented reasoning; HF datasets/models + GRPO training scripts.,scripts/train_still3.sh; scripts/eval_still3.sh; src/train.py; src/evaluate.py,"accelerate launch src/train.py --config configs/still3_train.yaml (source: README training pattern)
python src/evaluate.py --config configs/still3_eval.yaml (source: README evaluation pattern)","OpenAI API key (for specific tool-augmented pipelines, optional if using only HF models)",Uses Hugging Face datasets and model checkpoints; downloads occur on first run; user must set local paths if needed,Yes
PaddleMIX,https://github.com/PaddlePaddle/PaddleMIX,https://arxiv.org/abs/2503.04065,https://arxiv.org/pdf/2503.04065,README + tutorials,15.0,"Multimodal model-zoo (Qwen2/2.5-VL, InternVL2, LLaVA, SD3/SDXL/ControlNet); end-to-end training & inference pipelines (ppdiffusers); tooling: DataCopilot, PP-DocBee, PP-VCtrl.","examples/ folder scripts per model (e.g., examples/internvl2/run_infer_internvl2.py); cli/paddlemix_infer.py","python cli/paddlemix_infer.py --help (source: README → inference guide)
python examples/internvl2/run_infer_internvl2.py --model_name_or_path Qwen/Qwen2-VL-7B-Instruct --image_path demo.jpg --device cpu (source: examples/tutorials)",,Pretrained checkpoints auto-downloaded from PaddleHub or Hugging Face; optional datasets needed for training,Yes
Parlant,https://github.com/emcie-co/parlant/tree/arqs-a-systematic-method-for-optimizing-instruction-following-in-llms,https://arxiv.org/abs/2503.03669,https://arxiv.org/pdf/2503.03669,README,43.0,"ARQs for improved instruction following, modular architecture with MCP/OpenAPI tools, LLM via OpenAI with streaming, local vector DBs (ChromaDB, nano-vectordb), observability (OpenTelemetry/structlog), FastAPI+uvicorn/CLI, scheduling (croniter), pydantic-settings, S3/K8s/PostHog integrations, optional onnxruntime.",initialize_repo.py; scripts/analyze_results; package import: src/parlant,"pytest --no-cache (source: branch README)
pytest --no-cache --count=3 (requires pytest-repeat) (source: branch README)
python -m scripts.analyze_results parlant_test_results.jsonl (source: branch README mention of analyze_results)","OpenAI API key (core); optional: PostHog, S3, K8s, MCP/OpenAPI tool configs",No fixed dataset; uses prompts/tests; downloads only for optional integrations,Yes
LiteWebAgent,https://github.com/PathOnAIOrg/LiteWebAgent,https://arxiv.org/abs/2503.02950,https://arxiv.org/pdf/2503.02950,README,23.0,"Playwright agent with LLM (LiteLLM/OpenAI); LlamaIndex/Chroma integrations; multimodality (screenshots/image processing, STT via Deepgram); FastAPI service and demo scenarios; action scoring and logging; offline embeddings via HF.",function_calling_main.py; module: prompting_main; examples/google_test.py; load_state.py; module: api.server,"python test_installation.py (source: README → QuickStart)
python examples/google_test.py (source: README → QuickStart)
python -m prompting_main --agent_type PromptAgent --starting_url https://www.google.com
 --goal ""search dining table"" --plan ""search dining table"" --log_folder log (source: README → Development: try different agents)
python -m function_calling_main --agent_type FunctionCallingAgent --starting_url https://www.amazon.com/
 --goal ""add a bag of dog food to the cart."" --plan ""add a bag of dog food to the cart."" --log_folder log (source: README → Development: function-calling agent)
python -m api.server --port 5001 (source: README → Use LiteWebAgent AI backend)","OpenAI API key (required); optional: Deepgram, HF tokens for models/embeddings",No fixed dataset; downloads browser and optional model assets; user-provided URLs/tasks,Yes
EAGLE,https://github.com/SafeAILab/EAGLE,https://arxiv.org/abs/2503.01840,https://arxiv.org/pdf/2503.01840,README,21.0,A security-oriented framework for LLM assessment/enhancement; ready-made red-teaming scenarios and test suites; a single backend to OpenAI/Anthropic and local models via Transformers; web UI on Gradio; configs for reproducible pipelines; logging in W&B; modular plugin architecture.,eagle.application.webui; eagle/traineagle3/main.py; eagle/evaluation/gen_ea_answer_llama3chat.py; eagle/evaluation/gen_baseline_answer_llama3chat.py,"python -m eagle.application.webui --ea-model-path yuhuili/EAGLE3-LLaMA3.1-Instruct-8B --base-model-path meta-llama/Llama-3.1-8B-Instruct --model-type llama3 --total-token -1 (source: README Inference → With UI)
cd eagle/traineagle3 && deepspeed main.py --deepspeed_config ds_config.json (source: README Train)
python -m eagle.evaluation.gen_ea_answer_llama3chat --ea-model-path yuhuili/EAGLE3-LLaMA3.1-Instruct-8B --base-model-path meta-llama/Llama-3.1-8B-Instruct --use_eagle3 (source: README Evaluation)
python -m eagle.evaluation.gen_baseline_answer_llama3chat --ea-model-path yuhuili/EAGLE3-LLaMA3.1-Instruct-8B --base-model-path meta-llama/Llama-3.1-8B-Instruct (source: README Evaluation)",Hugging Face access token for downloads (optional); optional Weights & Biases API key,No fixed dataset; downloads pretrained checkpoints; MT-bench and model assets fetched on first run,Yes
Marco-o1,https://github.com/AIDC-AI/Marco-o1,https://arxiv.org/abs/2503.01461,https://arxiv.org/pdf/2503.01461,README,19.0,"two inference modes (via Transformers locally and via vLLM server), ready-made scripts/clients for chat and launch (src/main.py, src/infer/talk_with_model*.py, examples/*), modular tree-search pipeline (evaluator/utils) and lightweight HTTP endpoint.",src/main.py; src/infer/talk_with_model.py; src/infer/talk_with_model_vllm.py; src/infer/http_server.py,"python src/infer/talk_with_model.py --model_path AIDC-AI/Marco-o1 --prompt ""Hello"" (source: README local Transformers mode)
python -m vllm.entrypoints.api_server --model AIDC-AI/Marco-o1 --host 0.0.0.0 --port 8000 ; python src/infer/talk_with_model_vllm.py --api_base http://127.0.0.1:8000
 --prompt ""Hello"" (source: README vLLM mode)",Optional Hugging Face token for model downloads,No fixed dataset; large model checkpoints downloaded on first run,Yes
DCVC-RT,https://github.com/microsoft/DCVC,https://arxiv.org/abs/2502.20762,https://arxiv.org/pdf/2502.20762,README,8.0,"real-time learned video codec DCVC-RT with CPU/GPU modes, C++/pybind11 acceleration (entropy/motion), simple JSON configs and ffmpeg-compatible I/O, QP management by profiles and metrics/bitstream unloading.",test_video.py ,"python test_video.py --config configs/DCVC-RT/DCVC-RT.json --input input.mp4 --output output.bit --ckpt checkpoints/dcvc_rt.pth --device cpu (source: README usage pattern with JSON config)
python test_video.py --config configs/DCVC-RT/DCVC-RT.json --input output.bit --output recon.mp4 --ckpt checkpoints/dcvc_rt.pth --device cuda:0 (source: README usage pattern with JSON config)",,No fixed dataset; requires user-provided videos; pretrained checkpoints downloaded or placed locally,Yes
HVI-CIDNet,https://github.com/Fediory/HVI-CIDNet,https://arxiv.org/abs/2502.20272,https://arxiv.org/pdf/2502.20272,README,15.0,"ready-made weights with HF by name (--path), simple CLI for one image with handles --alpha_s/--alpha_i/--gamma, there is a Gradio demo (app.py, you can use --cpu), pure PyTorch without custom CUDA cores.",eval_hf.py; app.py ,"python eval_hf.py --path Fediory/HVI-CIDNet --img_path input.jpg --out_path output.jpg --alpha_s 0.5 --alpha_i 0.5 --gamma 0.5 (source: README CLI usage)
python app.py --cpu (source: README Gradio demo)",,No fixed dataset; single-image inputs; pretrained weights auto-downloaded from Hugging Face,Yes
UniDepth,https://github.com/lpiccinelli-eth/unidepth,https://arxiv.org/abs/2502.20110,https://arxiv.org/pdf/2502.20110,README,24.0,"Universal mono-DEPTH (UniDepth V1/V2) from one image/video; ready-made weights via HuggingFace; support for different backbones (timm), FOV-aware and scale-aware predictions, CPU-fallback mode, optional CUDA-accelerations (xformers, NystromAttention, KNN) for speed, simple demo scripts/demo.py and download .safetensors (~1.42 GB)",demo.py,"python demo.py --help (source: README demo usage)
python demo.py --input path/to/image.jpg --output depth.png --device cpu --weights lpiccinelli/unidepth-v2 (source: README pattern for HF weights)",,No fixed dataset; user-provided images/videos; pretrained weights auto-downloaded (~1.42 GB),Yes
GIFNet,https://github.com/AWCXV/GIFNet,https://arxiv.org/abs/2502.19854,https://arxiv.org/pdf/2502.19854,README,8.0,"Universal Infrared + Visible (IVIF) with simple test script; --VIS_IS_RGB flag for visible RGB; runs on CPU (slower, but no CUDA/3rd ops).",test.py,python test.py --VIS_IS_RGB True (source: README usage pattern),,"User-provided test images under images/IVIF/{ir,vis}; no fixed dataset auto-downloads",Yes
KAG,https://github.com/OpenSPG/KAG,https://arxiv.org/abs/2502.19209,https://arxiv.org/pdf/2502.19209,README,30.0,"KAG is a ready-made stand via Docker Compose (server, MySQL/MariaDB, Neo4j, MinIO) with plugin indexes (chunk/table/summary/outline/atomic/hybrid), hybrid search (graph+keyword), MinIO object storage, REST-API on :8887, background jobs and calling Python from Java via Pemja.","docker-compose-west.yml (services: server, mysql/mariadb, neo4j, minio)","docker compose -f docker-compose-west.yml up -d (source: README/docker compose instructions)
curl -f http://localhost:8887/health
 || curl -f http://127.0.0.1:8887/health
 (source: README API health check)",,,Yes
FinTSB,https://github.com/TongjiFinLab/FinTSB,https://arxiv.org/abs/2502.18834,https://arxiv.org/pdf/2502.18834,README,6.0,unified benchmark for forecasting/classifying financial series; YAML configs; ready-made baselines on PyTorch/Sklearn (LSTM/GRU/Transformer/XGBoost); standardized CSV/Parquet loaders; MSE/RMSE/MAPE/DA metrics; cross-validation and logging.,train.py,python train.py --config configs/forecasting/example.yaml (source: README usage pattern with YAML config),,Uses CSV/Parquet datasets; some require manual preparation/download; paths set in YAML,Yes
BatteryLife,https://github.com/Ruifeng-Tan/BatteryLife,https://arxiv.org/abs/2502.18807,https://arxiv.org/pdf/2502.18807,README,13.0,"Unified battery life prediction pipeline, base models (LSTM/GRU/TCN/Transformer), modular preprocessing, RMSE/MAE/R² metrics, experiment configs and logging.",run_main.py; evaluate_model.py; finetune.py; preprocess_scripts.py; domainAdaptation.py; train_eval_scripts/*.sh,"python preprocess_scripts.py (source: README → Preprocessing)
sh ./train_eval_scripts/CPTransformer.sh (source: README → Train the model)
sh ./train_eval_scripts/evaluate.sh (source: README → Evaluate the model)",,Requires processed datasets under ./dataset or downloads from Hugging Face/Zenodo as per README; raw datasets may need manual retrieval per tutorial,Yes
AISafetyLab,https://github.com/thu-coai/AISafetyLab,https://arxiv.org/abs/2502.16776,https://arxiv.org/pdf/2502.16776,README,14.0,"Modular architecture (attack/defense/scorers), ready-made attacks (AutoDAN) and simple defenses, single CLI+YAML configs, OpenAI/Transformers support, results logging and reproducible runs.",run_autodan.py,python run_autodan.py --help (source: README script usage),OpenAI API key (for OpenAI backend),No fixed dataset; uses prompts/configured tasks; model checkpoints downloaded as needed,Yes
SMARTS,https://github.com/huawei-noah/SMARTS,https://arxiv.org/abs/2502.15824,https://arxiv.org/pdf/2502.15824,README,21.0,"SUMO traffic simulator with multi-agent mode, scripts/DSL, headless and GUI, rich sensors, history-replay, parallel environment launch and integration with Gym/RLlib/SB3.",examples/e2_single_agent.py; examples/e3_multi_agent.py; examples/e8_parallel_environment.py; module: envision.server (UI),"scl run --envision examples/e3_multi_agent.py scenarios/sumo/loop (source: Docs → Quickstart)
PYTHONPATH=. python examples/e2_single_agent.py --headless --episodes 1 --scenarios scenarios/sumo/loop (source: Docs → Base Examples)",,Scenarios included under scenarios/; no large datasets required,Yes
LightThinker,https://github.com/zjunlp/LightThinker,https://arxiv.org/abs/2502.15589,https://arxiv.org/pdf/2502.15589,README,11.0,Anchor-Thought reasoning; model-agnostic pipeline on HF (Qwen/LLaMA etc.); CPU-smoke-test on small models (e.g. Qwen2.5-0.5B-Instruct); JSON configs for strategy/chunks/cache; PEFT/LoRA support; JSONL output and evaluation scripts (GPQA/GSM8K/CSQA).,local.py; scripts/inference.sh; evaluation/evaluate.py,"python local.py --model_name Qwen/Qwen2.5-0.5B-Instruct --config configs/anchor_thought_example.json --device cpu (source: README local inference)
bash scripts/inference.sh (source: README launcher)
python evaluation/evaluate.py --pred_path outputs/predictions.jsonl --dataset gsm8k (source: README evaluation)",Optional Hugging Face token for gated models,Uses HF datasets for GPQA/GSM8K/CSQA; downloads on first run; user-provided evaluation JSONL paths,Yes
Helical,https://github.com/helicalAI/helical,https://arxiv.org/abs/2502.13785,https://arxiv.org/pdf/2502.13785,README,19.0,"built-in HelixmRNA model for sequence embeddings, native work with AnnData/Scanpy and scIB, ready-made clustering (Louvain/Leiden) and dimensionality reduction (UMAP) pipelines, configurations via Hydra, Accelerate support for distributed/multi-device execution, integration with HuggingFace Datasets, optional unloading to Azure Blob, reproducible configs and logging.",helix_mrna_infer.py ,"python helix_mrna_infer.py --help (source: README usage pattern)
python helix_mrna_infer.py --input data/example.h5ad --output outputs/embeddings.h5ad --device cpu (source: README usage pattern)",Optional Azure Blob credentials for remote storage; optional Hugging Face token for gated datasets,Single-cell RNA datasets (AnnData .h5ad) or HF datasets; user-provided inputs; no auto-downloads by default,Yes
CBMNet,https://github.com/intelpro/CBMNet,https://arxiv.org/abs/2502.13716,https://arxiv.org/pdf/2502.13716,README,12.0,"joint processing of event data and RGB; ready-made checkpoint (checkpoints/cbmnet_pretrained.pth); demo script run_samples.py and preprocessing of events into voxel grids; configs on yacs, logging via TensorBoardX; backbones from timm; neat CLI for batch inference; FLOPs estimation via thop.",run_samples.py; test_bsergb.py; train.py; tools/preprocess_events.py,"python run_samples.py --model_name ours --ckpt_path pretrained_model/ours_weight.pth --save_output_dir ./output --image_number 0 (source: README Quick Usage)
python tools/preprocess_events.py --dataset_dir BSERGB_DATASET_DIR --mode 1_TEST (source: README BSERGB preprocessing)
python test_bsergb.py --dataset_dir BSERGB_DATASET_DIR (source: README BSERGB test)",,Requires ERF-X170FPS or BSERGB datasets; BSERGB needs voxel preprocessing and directory layout; pretrained weights must be downloaded into ./pretrained_model,Yes
MetaDE,https://github.com/EMI-Group/metade,https://arxiv.org/abs/2502.10470,https://arxiv.org/pdf/2502.10470,README,9.0,GPU-accelerated meta-evolution that automatically selects strategies and hyperparameters for Differential Evolution; two backends (JAX and PyTorch); integration with EvoX; ready-made examples for CEC benchmarks and Brax-RL; scalable batch runs,run_metade.py; examples/cec/run_cec.py; examples/brax/run_brax.py,"python run_metade.py --backend jax --config examples/cec/config.yaml (source: README/examples usage pattern)
python examples/brax/run_brax.py --env ant --backend torch --steps 100000 (source: README/examples usage pattern)",,,Yes
RAG-FiT,https://github.com/IntelLabs/RAG-FiT/tree/square,https://arxiv.org/abs/2502.09390,https://arxiv.org/pdf/2502.09390,README,14.0,"Hydra configs for three pipelines (processing / inference / evaluation); ready-made CoT/RAR and “baseline/-rag” presets for ASQA/HotpotQA/TriviaQA; default HF models (Phi-3-mini-128k-instruct) with 8-bit/4-bit loading and optional LoRA; built-in preprocessing steps (few-shot, join/flatten, prompts, upload to HF Hub); support for FAISS and external vector storage (Haystack/Qdrant); EM/F1/ROUGE/BERTScore/semantic + faithfulness/relevancy metrics; configurable prompts and response processors (regex); unified JSONL input/output.",processing.py; inference.py; evaluation.py; training.py ,"python inference.py pipeline=square task=asqa preset=rar model.name=Phi-3-mini-128k-instruct quantization.load_in_8bit=true retriever.faiss.index_dir=./faiss_index data.input_path=./data/asqa.jsonl output.path=./outputs/asqa_pred.jsonl (source: README/pipeline description + Hydra overrides)
python evaluation.py task=asqa data.pred_path=./outputs/asqa_pred.jsonl metrics.em=true metrics.f1=true metrics.rouge=true (source: README/evaluation description + Hydra overrides)",Optional Hugging Face token; Weights & Biases API key required for training.py,ASQA/HotpotQA/TriviaQA via local JSONL or HF datasets; FAISS/Haystack/Qdrant index required for RAG scenarios; paths must be configured,Yes
contrastors,https://github.com/nomic-ai/contrastors,https://arxiv.org/abs/2502.07972,https://arxiv.org/pdf/2502.07972,README,11.0,"Mixture-of-Experts embeddings (nomic-embed-text-v2-moe), instruction tuning with search_query:/search_document: prefixes, ""matryoshka"" truncation of dimensions (e.g. 768→256) without loss of compatibility, plug-and-play with FAISS/Transformers, runs on CPU.",run_contrastors.py,"python run_contrastors.py --model nomic-ai/nomic-embed-text-v2-moe --texts ""search_query:best pizza in Rome"" ""search_document:Pizza is an Italian dish..."" --output embeddings.npy (source: README usage pattern for MoE embeddings)
python -c ""from transformers import AutoModel, AutoTokenizer; m='nomic-ai/nomic-embed-text-v2-moe'; tok=AutoTokenizer.from_pretrained(m, trust_remote_code=True); mod=AutoModel.from_pretrained(m, trust_remote_code=True); print('Loaded:', mod is not None)"" (source: README → Transformers plug-in usage)",Optional Hugging Face token for gated/large model pulls,No fixed dataset; user provides texts; large model weights auto-downloaded (~1.9 GB),Yes
DPSDA,https://github.com/microsoft/DPSDA,https://arxiv.org/abs/2502.05505,https://arxiv.org/pdf/2502.05505,README,17.0,"modular pipeline (preparation → private training/generation → evaluation), config management (Hydra/YAML) with fixed seeds, privacy budget accounting (ε,δ) and reports, plugin models/embedders and metrics, support for mixed precision and gradient checkpointing, ready-made scripts for reproducible experiments and logging (W&B/CSV).",run_contrastors.py ,"python run_contrastors.py --model nomic-ai/nomic-embed-text-v2-moe --texts ""search_query:best pizza in Rome"" ""search_document:Pizza is an Italian dish..."" --output embeddings.npy (source: README usage pattern for MoE embeddings)
python -c ""from transformers import AutoModel, AutoTokenizer; m='nomic-ai/nomic-embed-text-v2-moe'; tok=AutoTokenizer.from_pretrained(m, trust_remote_code=True); mod=AutoModel.from_pretrained(m, trust_remote_code=True); print('Loaded:', mod is not None)"" (source: README → Transformers plug-in usage)",Optional Hugging Face token for gated/large model pulls,No fixed dataset; user-provided texts; large model weights auto-downloaded (~1.9 GB),Yes
